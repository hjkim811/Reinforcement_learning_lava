{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0f21899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from lava_grid import ZigZag6x10\n",
    "from tqdm import tqdm\n",
    "\n",
    "class agent():\n",
    "    \n",
    "    def __init__(self, environment, epsilon=0.1, learning_rate = 0.1, gamma=1):\n",
    "\n",
    "        self.environment = environment\n",
    "        self.q_table = dict()\n",
    "        for x in range(environment.nS):\n",
    "            self.q_table[x] = {0:0, 1:0, 2:0, 3:0} # 0: left, 1: up, 2: right, 3: down\n",
    "\n",
    "\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        \n",
    "    def action(self):\n",
    "        \n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = random.randint(0, len(self.action_space)-1)\n",
    "        else:\n",
    "            q_values_of_state = self.q_table[self.environment.s]\n",
    "            max_value = max(q_values_of_state.values())\n",
    "            action = np.random.choice([k for k, v in q_values_of_state.items() if v == max_value])\n",
    "        return action\n",
    "\n",
    "    def learn(self, old_state, reward, new_state, action):\n",
    "\n",
    "        q_values_of_state = self.q_table[new_state]\n",
    "        max_q_value_in_new_state = max(q_values_of_state.values())\n",
    "        current_q_value = self.q_table[old_state][action]\n",
    "\n",
    "        self.q_table[old_state][action] = (1-self.learning_rate) * current_q_value + self.learning_rate * (reward + self.gamma * max_q_value_in_new_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27eb691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 100000/100000 [01:18<00:00, 1270.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: -1.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# default setting\n",
    "max_steps = 100\n",
    "stochasticity = 0 # probability of the selected action failing and instead executing any of the remaining 3\n",
    "no_render = True\n",
    "\n",
    "env = ZigZag6x10(max_steps=max_steps, act_fail_prob=stochasticity, goal=(5, 9), numpy_state=False)\n",
    "s = env.reset()\n",
    "done = False\n",
    "cum_reward = 0.0\n",
    "\n",
    "\"\"\" Your agent\"\"\"\n",
    "agent = agent(env)\n",
    "\n",
    "\n",
    "# moving costs -0.01, falling in lava -1, reaching goal +1\n",
    "# final reward is number_of_steps / max_steps\n",
    "for _ in tqdm(range(100000)):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.action()\n",
    "        ns, reward, done, i = env.step(action)\n",
    "        ns = env.s\n",
    "        agent.learn(s, reward, ns, action)\n",
    "        cum_reward += reward\n",
    "        s = ns\n",
    "print(f\"total reward: {cum_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d74eb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -0.06278484491219424,\n",
       " 1: -0.06251147649689917,\n",
       " 2: -0.05508455593487663,\n",
       " 3: -0.05333910879446433}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76429264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.14123503700152826, 1: -0.1412358247143763, 2: -0.13124046675639064, 3: -0.13123902584050587}\n",
      "1 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.14122738939170587, 1: -0.13123441379733244, 2: -0.9999999999999994, 3: -0.1212489157909597}\n",
      "2 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "3 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "4 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.94185026299696, 1: -0.02379995140152356, 2: -0.020312126389088803, 3: -0.026572541363859795}\n",
      "5 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.025329549558340922, 1: -0.02260655334142808, 2: -0.015475971869231336, 3: -0.024998363531790836}\n",
      "6 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.016509311279000165, 1: -0.014997954458475512, 2: -0.008627278082963511, 3: -0.016001182238655633}\n",
      "7 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.011081092801456825, 1: -0.002, 2: -0.007722987699000001, 3: -0.007995306702100002}\n",
      "8 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.0066953279000000025, 1: -0.004863900000000001, 2: -0.003981000000000001, 3: -0.0036490000000000003}\n",
      "9 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.004791, 1: -0.0039000000000000007, 2: -0.005675610000000002, 3: -0.005663900000000001}\n",
      "10 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.13123457124354346, 1: -0.1412308139180942, 2: -0.1212455262068094, 3: -0.121242433018459}\n",
      "11 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.13122862919819078, 1: -0.13123515224053467, 2: -0.9999999999999994, 3: -0.11125474660619425}\n",
      "12 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "13 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "14 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.9929303495098489, 1: -0.02558238865949506, 2: -0.0273319892306118, 3: -0.03778579371013273}\n",
      "15 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.030635556568256235, 1: -0.02371042025185809, 2: -0.01973045722201816, 3: -0.03733102595825342}\n",
      "16 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.02419219094587107, 1: -0.013561652829656783, 2: -0.01121104724253113, 3: -0.7941088679053511}\n",
      "17 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.008157085965373847, 1: -0.005195100000000001, 2: -0.006186739900000002, 3: -0.5695327900000001}\n",
      "18 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.00259951, 1: -0.0031649000000000004, 2: -0.006188051000000001, 3: -0.005870441000000001}\n",
      "19 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.004477000000000001, 1: -0.0060604410000000015, 2: -0.006141441000000002, 3: -0.003439}\n",
      "20 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.12123726602087485, 1: -0.13123147298066223, 2: -0.11125190245132527, 3: -0.11124677559247828}\n",
      "21 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.12122542457467227, 1: -0.12123062580739874, 2: -0.9999999999999994, 3: -0.10127188325652846}\n",
      "22 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "23 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "24 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.9999987489239286, 1: -0.032768818450483475, 2: -0.03792515928198718, 3: -0.049606482383024245}\n",
      "25 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.040289648256527824, 1: -0.02906499520273019, 2: -0.9999760947410011, 3: -0.04692585587916532}\n",
      "26 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "27 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "28 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.271, 1: -0.00209, 2: -0.0019000000000000002, 3: -0.003439}\n",
      "29 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.002, 1: -0.0029100000000000003, 2: 0, 3: -0.0019000000000000002}\n",
      "30 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.11124098386378706, 1: -0.12123165652605701, 2: -0.101264356881745, 3: -0.10125553537443249}\n",
      "31 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.11122883839523721, 1: -0.1112370884705257, 2: -0.9999999999999994, 3: -0.0912859508351439}\n",
      "32 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "33 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "34 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.9999999999999994, 1: -0.0418543087176014, 2: -0.04808681269990783, 3: -0.061047873863352746}\n",
      "35 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.05066635889775271, 1: -0.03836156688036322, 2: -0.9999999975020037, 3: -0.05763843057017165}\n",
      "36 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "37 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "38 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.19, 1: -0.0020900000000000003, 2: 0, 3: -0.001}\n",
      "39 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.001, 1: -0.001, 2: 0, 3: 0}\n",
      "40 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.10123603077394382, 1: -0.11123138243142922, 2: -0.09128100926304099, 3: -0.11106730144597769}\n",
      "41 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.10123235598337356, 1: -0.10125385769978673, 2: -0.08132085472335962, 3: -0.1011015959394069}\n",
      "42 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.09123606458185499, 1: -0.9999999999999994, 2: -0.07136865264103948, 3: -0.09108186813261983}\n",
      "43 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.08111121429003615, 1: -0.9999999999999994, 2: -0.06152113013313805, 3: -0.0811027366960356}\n",
      "44 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.0711675640837595, 1: -0.05165883266931042, 2: -0.05800746749687566, 3: -0.07123659542896851}\n",
      "45 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.06096915111854942, 1: -0.04810545889639965, 2: -0.9999999999999994, 3: -0.06768561763264049}\n",
      "46 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "47 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "48 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: -0.001}\n",
      "49 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "50 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.11104944333763764, 1: -0.10123156742819756, 2: -0.10110464899155637, 3: -0.11104689035537334}\n",
      "51 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.11100821193613612, 1: -0.09124943830719051, 2: -0.09118184560239972, 3: -0.10108605466630802}\n",
      "52 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.10105931804884864, 1: -0.08128311595945673, 2: -0.08125400624430357, 3: -0.09113751729344576}\n",
      "53 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.0911171932153035, 1: -0.07138325717927534, 2: -0.07132291383961176, 3: -0.08114685333310029}\n",
      "54 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.08103411433766669, 1: -0.061484099342784995, 2: -0.06782788082385527, 3: -0.07116872731821149}\n",
      "55 상태의 행동가치 함수는 다음과 같다\n",
      "{0: -0.07087466649616232, 1: -0.05790663308701141, 2: -0.9999999999999994, 3: -0.06775400390498519}\n",
      "56 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "57 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "58 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0.1, 3: -0.001}\n",
      "59 상태의 행동가치 함수는 다음과 같다\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(60):\n",
    "    print(i, '상태의 행동가치 함수는 다음과 같다')\n",
    "    print(agent.q_table[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd3969",
   "metadata": {},
   "source": [
    "### 7, 8, 17, 18 상태에서 루프를 도는 모습을 보임\n",
    "# 시간차 학습으로 해결 요망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b910c5c",
   "metadata": {},
   "source": [
    "1. agent에 env를 copy 해서 넣고, 실제 환경과 차이를 두고 시간차 학습\n",
    "2. epsilon을 점차 감소\n",
    "3. 벽에서 할 수 있는 행동을 제약"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
